# ğŸ“‘ Planning Document Text Extraction & Embedding Pipeline 

![generated-image (2) copy](https://github.com/user-attachments/assets/4ef6f28c-c38c-4ec4-b3d0-05a9e4f44aa1)

## ğŸ¯ Problem Statement  

The UK holds 12 million+ planning applications in diverse formats (PDF, TIF, DOCX, RTF, TXT)â€”plus noisy files (.msg, .html, .mp4, .jpeg, .gif). I needed to extract all text, generate sentence- and paragraph-level embeddings, and power land chatbot (LandGPT) with precise citations linked back to source docs.

## âš™ï¸ Technical Approach  

* S3 bucket ingestion with MIME-type filtering to weed out noise    
* Text extraction via pure-Python libs (pymupdf, docx, pypdandoc) to keep LLM extraction costs to a minimum  
* OCR of scanned PDFs/TIFs using Google Gemini Flash 2.0 (1M token context, multimodal, fast, incredibly cheap)    
* Embedding with open source nomic-embed-text model (768Â­-dimensional vectors) via ollama for cost-effective search   
* Storage in clientâ€™s Postgres DB, leveraging pgvector, pgvectorScale & pgai extensions    
* Scheduler-worker pattern with Docker \+ AWS SQS, horizontally scalable on Kubernetes spot instances    
* Environment management via Pydantic BaseSettings, modular clean code with idempotent upsert logic


## ğŸ›  Skills  

Python Â· AWS S3 Â· Google Gemini Flash 2.0 Â· ollama Â· nomic-embed-text Â· Postgres Â· pgvector Â· pgvectorscale Â· pgai Â· Docker Â· AWS SQS Â· Kubernetes Â· Pydantic Â· OCR Â· Embeddings Â· Clean Architecture Â· Cost Optimisation

## ğŸ”§ Challenges & Solutions  

â€¢ High noise ratio in S3 â†’ rigorous MIME-type & extension filtering    
â€¢ Scanned PDF/TIF files â†’ Gemini Flash Flash 2.0 OCR for reliable multimodal extraction    
â€¢ Massive scale & cost constraints â†’ open-source embeddings \+ spot-instance K8s scaling    
â€¢ Avoiding redundant work â†’ SQL-backed dedupe checks before extract/embed    
â€¢ Client DB consistency â†’ integrated with existing Postgres stack (deliberately didnâ€™t use a vector DB such as Qdrant to keep tech stack simple and maintainable)

## ğŸ“Š Quantifiable Business Impact  

â€¢ Processed 12 million docs (avg. 10 pages) in under 48 hours (â‰ˆ 250k pages/hr)    
â€¢ Reduced extraction & embedding cost by 65% vs. GPT-4 baseline    
â€¢ Enables daily ingestion of 1k+ new planning apps, keeping LandGPT data fresh    
â€¢ LandGPT platform engagement up 22%

## â­ Client Review

<img width="933" alt="Screenshot 2025-05-09 at 13 14 02" src="https://github.com/user-attachments/assets/9b0c4c3a-8882-42e9-be76-6e4b9f340ffe" />

*Note: this was one project of many from a long-term contract with the above client*
